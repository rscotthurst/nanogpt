{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f1b353-9a48-4e2c-a13d-6a0d90a38592",
   "metadata": {},
   "source": [
    "# Train a Transformer to Write Shakespeare\n",
    "\n",
    "This tutorial notebook is based on [this video](https://www.youtube.com/watch?v=kCc8FmEb1nYhttps://www.youtube.com/watch?v=kCc8FmEb1nY)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a9a0d-4ad7-4c9a-ac7e-031b56f08e2b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8252e7ec-d7bb-4ce9-90a2-97383460c6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a2373-2d45-47a0-8be7-2566b1717d45",
   "metadata": {},
   "source": [
    "## Download and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e57f241-deda-4e5b-8e88-02bd9fc25215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebbf299-6b88-4481-b666-2edf121ca45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d67c5d7-36a6-46c8-a2b0-86eaa4b071c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset in characters: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33af1dc-1a4a-474f-98b4-adb123214554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first 500 characters\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db8528f-3d2d-4fc7-ba08-305c0844cd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters in the text:\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'There are {vocab_size} unique characters in the text:')\n",
    "print(''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c400c7-1132-4bf3-af69-f9f55360d431",
   "metadata": {},
   "source": [
    "## Create a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b9fd0-c315-4a88-8c11-9c3b012b177a",
   "metadata": {},
   "source": [
    "Tokenizing is to put the text in a language the computer can understand, in this case a list of characters. We're doing something really simple, just assigning an integer to each of the possible 65 characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe4923-9744-4b5b-b7e9-fe0eec1d6e73",
   "metadata": {},
   "source": [
    "First we build the mapping ourselves by creating a lookup table in both directions, string to integer (`stoi`) and vice versa (`itos`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6565984d-317f-4967-a19e-83e1fe4664df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to Integer Mapping\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Integer to String Mapping\n",
    "itos = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd0e20-cf51-44dc-8a8a-c64868c78fc7",
   "metadata": {},
   "source": [
    "Now write two quick functions, one to encode (string to int) and one to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4040eec-426f-43a6-9fc4-03d5fef70fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create quick functions using lambda (lambda is used for one-liners like this)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111c211-b451-4379-a6b9-6767773437d9",
   "metadata": {},
   "source": [
    "Now test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e0bd48-8fc8-42a7-955e-c12930db5456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hii there!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there!\"))\n",
    "print(decode(encode(\"hii there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9254e-64a7-48db-b8ea-10224f82c72f",
   "metadata": {},
   "source": [
    "And test on the Shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa7fd07-65b2-4f86-9b54-158da09f9d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test encoding on first 15 characters of text\n",
    "encoded_txt = encode(text[:15])\n",
    "encoded_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ec86e1-e65e-4c3e-8391-8d9bf6a52d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now decode it\n",
    "decode(encoded_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419546f5-bf9c-41da-b56b-7fcda8fefc49",
   "metadata": {},
   "source": [
    "There are many different ways to build a tokenizer, we just did a very simple one. For example, Google created one called [SentencePiece](https://github.com/google/sentencepiece). It tokenizes at the sub-word level, which is usually adopted in practice for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2a805-c553-414d-97d2-ed586eb2836d",
   "metadata": {},
   "source": [
    "## Tokenize the Text\n",
    "\n",
    "We're going use our basic encoder to encode the entire text and store it into a `torch.Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7359bc35-919a-4ade-96a2-6c28dae02bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01611d69-048f-4c26-ba69-4e3609c02acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caade9-814b-4b32-b94d-e9bc8a7cf83b",
   "metadata": {},
   "source": [
    "## Split into Train/Validation Datasets\n",
    "\n",
    "90/10 split. This will help us understand how much our model is overfitting by hiding 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7ff0a3-7cab-49cb-aee3-4b6b8ed9ffe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))  # Determine cut point of 90%\n",
    "train_data = data[:n]  # First 90% is train\n",
    "val_data = data[n:]  # Last 10% is validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25c21-14a8-4117-8997-363612603895",
   "metadata": {},
   "source": [
    "## Concept of Block Size\n",
    "\n",
    "Feeding everything into the model would be computationally expensive and prohibitive. So we are going to separate it out into chunks of 8. But because we're interested not just in the character but the relationship from character to character, we're going to look at `block_size + 1`. This way we can go to up to 8 characters and see the following character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c8c674-5406-4268-91dc-808423b49432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54ea4f-f7e5-4682-b848-735fb0426e74",
   "metadata": {},
   "source": [
    "There are 8 examples of data embedded here. There's the first (18), which is followed by 47. There's the first AND second (18, 47) and followed by 56. We're going to refer to this dimension as the **time dimension**. Illustrating this with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538b23b3-2f4b-4049-8aeb-775b16db4563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af06a2-72d5-4e1d-92ca-79e0db3e1c2f",
   "metadata": {},
   "source": [
    "## Concept of Minibatches\n",
    "\n",
    "For effeciency we're going to batch multiple blocks together in batches (also called *minibatches*). They are processed separately and don't talk to each other. GPUs are really good at parellelizing. This is the **batch dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b62906c4-759a-49ba-85d5-25ededf9c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)  # Now we're setting a random seed because we're grabbing random samples in the data\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4  # B: How many independent sequences will we process in parallel?\n",
    "block_size = 8  # T: What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # create 4 random starting places in the text\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "957c72ed-442c-421b-8dd0-65c0480a8934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Inputs ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "== Targets ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "========================\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('== Inputs ==')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('== Targets ==')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('========================')\n",
    "for b in range(batch_size):  # Batch dimension\n",
    "    for t in range(block_size):  # Block dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00298-ad41-426a-a51a-ba1375d7780b",
   "metadata": {},
   "source": [
    "## Feed into a Neural Network\n",
    "\n",
    "Now we know what kind of input we want to feed to our transformer, it's an 8x4 tensor. We'll start with the simplest type of NN, a bigram languge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455dd0d0-92b6-4218-830a-23fa48fb5033",
   "metadata": {},
   "source": [
    "### Simplest Bigram Language Model\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1331s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61372162-958a-4fff-9fee-62025dfb89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f330652-7301-4067-92dd-2adb66a7e539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a6ea4-8915-4ee5-bdd8-5ee14faa0e29",
   "metadata": {},
   "source": [
    "So what is this doing? We're feeding our minibatch into it. See previous section to remind what `xb` and `yb` are. The embedding table is 65x65. As the `forward` method runs, every integer is going to pluck a row of the embedding table corresponding to its index. Then Pytorch arranges it as a *Batch x Time x Channel* tensor. So we have a 65-length tensor for every one of the 4 x 8 positions in our input set. \n",
    "\n",
    "The output are the `logits` or the scores for the next characters in the sequence. We're predicting what comes next based *only* on the location of the token in the sequence. Ie what are the odds of each character coming up in position 5 in the 1st minibatch?\n",
    "\n",
    "We can figure that out!\n",
    "\n",
    "First, use `torch.sort()` to sort the embedding table of the fifth position of the first minibatch (remember zero-indexing). Then recall the top 5 indices. Looks like our most likely character is 22. We can remind ourselves which character is index 22 in our vocab list. This really isn't going to be very accurate, we're not measuring loss, we're not optimizing anything. It's about as good as a random guess, as we'll see soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2b3344-de11-47f8-8cb1-5e2ac30d0e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1471, 1.9577, 1.3954, 1.1822, 1.1564], grad_fn=<SliceBackward0>)\n",
      "tensor([10, 55, 44, 64,  4])\n",
      "Index 10 is: \":\"!\n"
     ]
    }
   ],
   "source": [
    "sorted, indices = torch.sort(out[0][4], descending=True)\n",
    "\n",
    "print(sorted[:5])\n",
    "print(indices[:5])\n",
    "print(f'Index {indices[0]} is: \"{chars[indices[0]]}\"!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45dee8-b2db-4b0c-ac48-9db3467844c7",
   "metadata": {},
   "source": [
    "### Add Loss\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1500s)\n",
    "\n",
    "Now that we've made predictions about what comes next, let's evaluate them with a **loss function**. A good way to measure the loss, or quality of the prediction is to use *negative log likelihood loss*, which is implemented in PyTorch under the function `cross_entropy()`. Loss is the cross entropy of the predictions and the targets. We measure the quality of the logits with respect to the targets. We have the identity of the next character, so we want to know how well we're predicting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f620d7f-5ccb-4ad5-adf5-198785ed873d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        # Loss is cross entropy, we have to reshape the input though per documentation\n",
    "        # It wants B x C x T rather than B x T x C, so we'll reshap logits\n",
    "        B, T, C = logits.shape # unpack\n",
    "        logits = logits.view(B*T, C)  # Stretch them out into 1-dim sequence, C is 2nd dim\n",
    "        \n",
    "        # Also need to do same to targets\n",
    "        targets = targets.view(B*T)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bebc8f1-5a29-43be-a118-8a9a65205793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c3d4d-5f06-4f24-8ce2-59e2e616da18",
   "metadata": {
    "tags": []
   },
   "source": [
    "It should be obvious now how we \"stretched\" logits to make it work for the cross_entropy function. Instead of 4 x 8 x 65 we have 32 x 65. Let's look at the first one, which would have corresponded to the first index of the first minibatch. It's a 65-len tensor giving all my logits. I can again sort it to figure out the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "085a84de-81a7-42b3-baf8-380de91c812d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4849, -0.8661, -1.0219, -1.1038, -0.3763, -0.2251, -0.2825,  0.4622,\n",
      "        -0.7136, -1.4831,  1.3023, -1.2810,  1.2857,  0.8153, -1.4935, -0.1127,\n",
      "         0.7719,  2.9956,  0.2084, -1.6306,  1.4533, -1.1483,  0.7007,  1.2882,\n",
      "         0.7806,  1.2904,  0.0471,  1.4801, -0.6316, -1.1766, -1.5717,  0.5684,\n",
      "         1.2815,  0.4047,  0.0632,  0.5846, -0.1738,  0.8185, -0.5315, -0.7415,\n",
      "         0.6128,  0.9535, -0.0584, -0.4370,  0.2026, -0.8318, -0.1020,  0.9157,\n",
      "        -0.6446, -0.5180,  0.8405, -1.3159,  0.0663, -0.7541,  0.7109, -0.3921,\n",
      "        -1.4153, -0.0123,  0.2143,  1.5742, -1.7377,  0.9368,  0.1410,  1.5414,\n",
      "         0.7376], grad_fn=<SelectBackward0>)\n",
      "tensor([2.9956, 1.5742, 1.5414, 1.4849, 1.4801], grad_fn=<SliceBackward0>)\n",
      "tensor([17, 59, 63,  0, 27])\n",
      "Index 17 is: \"E\"!\n"
     ]
    }
   ],
   "source": [
    "print(logits[0])\n",
    "\n",
    "# Sort\n",
    "sorted, indices = torch.sort(logits[0], descending=True)\n",
    "\n",
    "# Output Results\n",
    "print(sorted[:5])\n",
    "print(indices[:5])\n",
    "print(f'Index {indices[0]} is: \"{chars[indices[0]]}\"!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989b6a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7288, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725bd10",
   "metadata": {},
   "source": [
    "Our loss is stated in the `loss` tensor, output in the previous cell. This will change at each run but when I wrote this it was 4.7288. We can calculate the expected loss as the negative natural logarithm (hey look, NEGATIVE LOG LIKELIHOOD!?) of 1 over the number of dimensions. In our case that is $-{\\ln}\\frac{1}{65} = 4.174$. The fact that our loss is higher than expected means our model is NOT diffuse. We have entropy, we're guessing wrong. We want much lower loss than the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582c217",
   "metadata": {},
   "source": [
    "### Add Generation\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=28m50s)\n",
    "\n",
    "Still, despite our garbage model, we're still going to try some **generation**! Lets add that to our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8aae16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss is cross entropy, we have to reshape the input though per documentation\n",
    "            # It wants B x C x T rather than B x T x C, so we'll reshap logits\n",
    "            B, T, C = logits.shape # unpack\n",
    "            logits = logits.view(B*T, C)  # Stretch them out into 1-dim sequence, C is 2nd dim\n",
    "\n",
    "            # Also need to do same to targets\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indeces in the current context. The job of generate is to \n",
    "        # take the B x T array and extend it to be B x T +1, +2, +3, etc. up until\n",
    "        # max_new_tokens is reached.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # For now we ignore loss, we're only using logits\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14168eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.2793, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79736d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1x1 tensor holding a zero to kick off the generation\n",
    "# Remember a 0 corresponds to a newline (\\n) character\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "736279d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MufR3!yC NHKGY?C3eDEGMp-VCcZN3aoRN&powfXmB.EOX.aFl3sitGj&WcGpoEfRvHLbR?\n",
      "JxHofRX,MccAWBTF.ALF$mQ;!GY:\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0a638",
   "metadata": {},
   "source": [
    "### WHAT!? This is Garbage!\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2093s)\n",
    "\n",
    "It's garbage because this model was never trained, not at all. Now let's actually train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "435cf0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pytorch optimization object using AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3edaa3",
   "metadata": {},
   "source": [
    "The optimizer we just created will take the gradients and update the parameters using the gradients. Another possible optimizer would be Stochastic Gradient Descent (`torch.optim.SDG`).\n",
    "\n",
    "Now let's set it all up and run with just 10 iterations and see how much loss we get. Now we're actually training the model, we're feeding the data into our model and updating parameters using our `optimizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9897a4c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6318359375\n",
      "4.627881050109863\n",
      "4.483257293701172\n",
      "4.629354476928711\n",
      "4.51035213470459\n",
      "4.547611713409424\n",
      "4.6271467208862305\n",
      "4.50986385345459\n",
      "4.6163458824157715\n",
      "4.603818893432617\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e276ab9",
   "metadata": {},
   "source": [
    "It's getting lower, but lets train more next. We'll do 10k iterations, and only print loss at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d78971d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.52305006980896\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "print(f'final loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1637cf4",
   "metadata": {},
   "source": [
    "Our loss is now greatly reduced. Now lets get predictions again. It will look a little better, but it's not shakespeare yet! Not even readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "682af826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dugomyof y, beche iberJubs R; thathagutth sedy t jPcant my thes hararis?\n",
      "Parared u, waviteavedo winegot Wied fon:\n",
      "LIUD wigotho f beaind ailit bhill spanone w;\n",
      "Thad f yor m h avingease'r prG t laresothord, s d thept en, char pou;\n",
      "te'd is!pe; tadsplout\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be3f1",
   "metadata": {},
   "source": [
    "We could even train 10k more times and see if that helps, probably not though, because we've likely reached some kind of maximum given the simplicity of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b39ea42a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.405074119567871\n",
      "\n",
      "\n",
      "LUCisurl mar ad\n",
      "Whith?\n",
      "Whewid m S:\n",
      "\n",
      "Hurt ges d at fu.\n",
      "Ance y h y Wesomy MILOLLAnouceand or at is des\n",
      "A: buthivequchen thy? wit ay t hinsofo youe r ar p gromig: twiglillldica dritond\n",
      "NBusthouth:\n",
      "Whaf be ssothary:\n",
      "E ou sour'dod\n",
      "\n",
      "In AKINUSuseaze.\n",
      "\n",
      "Mas \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "print(f'final loss: {loss.item()}')\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fe701",
   "metadata": {},
   "source": [
    "Yeah... training this over and over isn't helping, there's just only so much such simple architecture can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068a28b-d2cf-4ef4-b7e1-b7e0cafdb526",
   "metadata": {},
   "source": [
    "# Next... Increase complexity of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a312b69-8ae9-46a3-b5ff-254a81fd5bdf",
   "metadata": {},
   "source": [
    "Our model up to this point has been incredibly simple, the tokens aren't talking to each other. Though we're training on blocks of up to 8, we're only using the last character to figure out what comes next. Now we're going to get these tokens talking to each other in a more optimized way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d8027-bfe8-4c3b-bb95-8d346734a67c",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention\n",
    "\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2533s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0b14105-cd88-4951-87e2-ee72c54e9ae9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consider the following toy example with random data in a 4x8x2 array:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa7b83-3aa8-4d3b-8408-781bf5cd98aa",
   "metadata": {},
   "source": [
    "Currenty the 8 tokens in a batch (aka time) are not talking to each other. We want them to talk to each other. But we only want tokens talking to the token that comes before them. AKA, token in position 5 can't see tokens 6, 7, 8 but can see tokens 1-4. We want to do a rolling average but not allow tokens to look into the future. A rolling average isn't great, we lose information but that's fine for this example.\n",
    "\n",
    "So for every *nth* step, we want to capture avereage of the *nth* and previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a1793-3ae1-4ab7-81d7-97b4c1310307",
   "metadata": {},
   "source": [
    "### Version 1: For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78219ada-6b3d-4108-81fe-53653c8da3ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# bow = bag of words, there's a word stored at every one of the T positions\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    # print('======')\n",
    "    # print(b)\n",
    "    for t in range(T):\n",
    "        # print(f'  ->{t}')\n",
    "        xprev = x[b, :t+1]\n",
    "        # print(f'    {xprev}')\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "print(x.shape)\n",
    "print(xbow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5f6c4-7dbd-41f8-b8c8-c9fbdb5c0d4b",
   "metadata": {},
   "source": [
    "#### Look at the output!\n",
    "Now it starts to make sense. Let's start by comparing `x[0]` to `xbow[0]`. The first line is equal because it's just an average of itself, but the second line is now the average of the two because $(0.1808 + -0.3596) / 2 = -0.0894$ and $(-0.0700 + -0.9152) / 2 = -0.4926$. Then the third position is the average of the three, and so on. The last element is the vertical average of all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2658e4d2-97a1-4ff8-84e4-cf7e0106daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98046053-b50f-4266-8c6d-1e9e155a2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3e627-7992-4027-aec7-8b0201a289ba",
   "metadata": {},
   "source": [
    "### Version 2: Matrix Multiplication\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2831s)\n",
    "\n",
    "This is great BUT for loops are inefficient. Would be much better to do with Matrix multiplication. Lets look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "618d6540-6b57-45f0-9b16-34715f7d08ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.ones(3, 3)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9109-4609-4f20-a2d3-adbcd86b980f",
   "metadata": {},
   "source": [
    "The **dot product** in position 1 of tensor `c` (value of 14) comes by multiplying the first **row** of tensor `a` by the first **column** of tensor `b`. Since `a` is all ones it ends up just being a sum of `a` column 1. See here for [dot product refresher](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-matrices/a/multiplying-matrices).\n",
    "\n",
    "But `torch` has another great tool called `torch.tril()` that gives just the lower triangular values of a tensor and zeroes out the rest. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acb09821-2541-4efe-9555-57ddc1f0556b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones, wrap in tril to zero out the top right and give running sums\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7b1a6-ec20-428a-969a-ef79a164d4e8",
   "metadata": {},
   "source": [
    "When we do this we end up with a **running sum**! To make them averages only takes a little more effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c720f549-0ee3-497f-a46b-4c0864bf5b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a94c42-851f-4da7-ac30-53c22cac99ea",
   "metadata": {},
   "source": [
    "Now we can vectorize this and scale to our previous example with `x` and `xbow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1205d56e-3596-4688-bba2-c293449b74fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8b5c4-e17e-41fb-a835-b698bdd86bbf",
   "metadata": {},
   "source": [
    "### Version 3: Softmax\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3282s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f578d7-cff4-4aad-a503-236d25d60613",
   "metadata": {},
   "source": [
    "Softmax is a normalization operation, so we get the same matrix. The reason we want to use *this one* is because the weights will start with zero but eventually the relationships will not be constant at zero but will be data dependent. Now we'll start to call these **affiinities**, some values find others more interesting and that is the **basis of attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c73f358-db55-4828-b496-dabd8c87ab00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix and set aside\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6fba7dca-6082-47dc-9367-f94a0df72137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T, T))  # Initialize wei as all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is how we keep the past from seeing the future\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654082b-8ddb-481b-acc6-dddddfd83de0",
   "metadata": {},
   "source": [
    "If we do a softmax across every row of `wei`, what does it do? Softmax is a normalization operation, it *exponentiates* each element along a row ($e^x$), and then divides by the sums of those exponents. $e^0 = 1$ so we get 1 every time the `tril` tensor has a zero. Likewise $e^\\text{-Â inf} = 0$.\n",
    "\n",
    "With this in mind, we can calculate each row below by altering the `wei_idx` variable in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "310ea813-a0e5-4a6a-87b5-7d5e016bd266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 of wei is:                tensor([0., 0., -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "Exponentiated that row becomes: tensor([1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "The sum of the exponents is:    2.0\n",
      "So softmax is:                  tensor([0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "wei_idx = 1\n",
    "\n",
    "print(f'Row {wei_idx} of wei is:                {wei[wei_idx]}')\n",
    "print(f'Exponentiated that row becomes: {torch.exp(wei[wei_idx])}')\n",
    "print(f'The sum of the exponents is:    {torch.sum(torch.exp(wei[wei_idx]))}')\n",
    "print(f'So softmax is:                  {torch.exp(wei[wei_idx]) / torch.sum(torch.exp(wei[wei_idx]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0e53e-13bd-45ce-971e-2ed47debfb16",
   "metadata": {},
   "source": [
    "Fortunately pytorch includes a softmax function, we just need to tell it along which dimension to calculate it and we can calculate softmax for each of our 8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1b90f51-394d-4528-80c8-5e5905ec1e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a softmax along every row (because dim = -1), softmax is a regularization\n",
    "# function so it does the same as in version 2.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596075a3-b1cf-412f-aafa-60bb95a38e62",
   "metadata": {},
   "source": [
    "And with that creatively devised mask in place, we can find the dot product and once again we get our nice rolling average.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ea5fa56-07c1-4dff-b3a9-0ff511581559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bd0e9-cf7d-4d8e-b1ec-c1250883651c",
   "metadata": {},
   "source": [
    "#### Why do we care about softmax and all these different methods of doing a rolling average!?\n",
    "The reason we'll use this in self-attention (next), is because we will start changing the affinities. Up until now we've made them all zero (we initialized `wei` with `torch.zeros()`). Using softmax, we can do weighted self-attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574678b1-59fd-4dfc-b2a4-4987c3264d92",
   "metadata": {},
   "source": [
    "### Version 4: Self-Attention!\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3720s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d39800-fee5-42a1-9560-6256c4e38fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # Now C is 32\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fc861-4185-4f8c-ad07-9d5d142d2db4",
   "metadata": {},
   "source": [
    "Previously we initiliazed `wei` with all zeros. We need to change this, because we don't want all zeros because some tokens will have more/less affinity for others. If we initialize as zero then `wei` has uniform rows, but that's not what we want in real life. Some tokens will find other tokens more/less interseting and we want this to be data-dependent.  Ex., a vowel might be looking for consanants in past, we want to know what they are and flow to the next token.\n",
    "\n",
    "How we solve: Every token at each position will emit TWO VECTORS. A query and a key vector: What am I looking for? Key vector: What do I contain. The affinities come by doing a dot product with keys and queries. **That dot product becomes `wei`.** If key/query are aligned, they interact to a higher degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36c5cd-953b-4f06-a72c-fc73da9818f5",
   "metadata": {},
   "source": [
    "#### Let's see a single head perform self-attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb0b04-b261-4903-83e7-f645febae4db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_size = 16  # Hyperparameter for head size\n",
    "key = nn.Linear(C, head_size, bias=False)  # Initialize linear modules with bias=False to avoid getting fixed weights\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Produce k and q by forwarding key and query on x\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "print(k[0])\n",
    "print(q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27956bd-7cdf-4583-911a-e9ba3b2bd9eb",
   "metadata": {},
   "source": [
    "When we forwarded `key` and `query` on `x`. Each token in all positions in BxT arrangement produce a key and query. No communication has happened yet. That happens now. All the queries will dot product with each of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51205fea-4085-49cd-83a6-43a12250334e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need to transpose to the right dimension before dot product. What we get is:\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf55d8-97d7-4159-911e-349c6c294d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now add the other steps in to remove the top triangle of data (otherwise we're leaking data)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix and set aside\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is how we keep the past from seeing the future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f27a3-443b-4398-9b5c-140facb625c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For every row of B we'll have a T^2 matrix giving us the weight.\n",
    "# The weights are no longer uniform like they were before but indicate\n",
    "# affinities between tokens.\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a9b3f-cbdc-41ed-a332-110b58b24722",
   "metadata": {},
   "source": [
    "## NB -> Script Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3894e55-8d47-44a9-aa89-db16605a2d9d",
   "metadata": {},
   "source": [
    "This will be easier if we work from a script instead of a notebook. Use version control to see the various steps we took to go from a basic implementation to something much more fully featured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e9550-5392-4e43-82fd-31a83a228bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
