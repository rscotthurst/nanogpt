{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f1b353-9a48-4e2c-a13d-6a0d90a38592",
   "metadata": {},
   "source": [
    "# Train a Transformer to Write Shakespeare\n",
    "\n",
    "This tutorial notebook is based on [this video](https://www.youtube.com/watch?v=kCc8FmEb1nYhttps://www.youtube.com/watch?v=kCc8FmEb1nY)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a9a0d-4ad7-4c9a-ac7e-031b56f08e2b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8252e7ec-d7bb-4ce9-90a2-97383460c6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a2373-2d45-47a0-8be7-2566b1717d45",
   "metadata": {},
   "source": [
    "## Download and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e57f241-deda-4e5b-8e88-02bd9fc25215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebbf299-6b88-4481-b666-2edf121ca45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d67c5d7-36a6-46c8-a2b0-86eaa4b071c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset in characters: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33af1dc-1a4a-474f-98b4-adb123214554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first 1k characters\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db8528f-3d2d-4fc7-ba08-305c0844cd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters in the text:\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'There are {vocab_size} unique characters in the text:')\n",
    "print(''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c400c7-1132-4bf3-af69-f9f55360d431",
   "metadata": {},
   "source": [
    "## Create a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b9fd0-c315-4a88-8c11-9c3b012b177a",
   "metadata": {},
   "source": [
    "Tokenizing is to put the text in a language the computer can understand, in this case a list of characters. We're doing something really simple, just assigning an integer to each of the possible 65 characters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6565984d-317f-4967-a19e-83e1fe4664df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to Integer Mapping\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Integer to String Mapping\n",
    "itos = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4040eec-426f-43a6-9fc4-03d5fef70fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create quick functions using lambda (lambda is used for one-liners like this)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa7fd07-65b2-4f86-9b54-158da09f9d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test encoding on first 15 characters of text\n",
    "encoded_txt = encode(text[:15])\n",
    "encoded_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ec86e1-e65e-4c3e-8391-8d9bf6a52d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now decode it\n",
    "decode(encoded_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2a805-c553-414d-97d2-ed586eb2836d",
   "metadata": {},
   "source": [
    "## Tokenize the Text\n",
    "\n",
    "We're going to encode the entire text and store it into a `torch.Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7359bc35-919a-4ade-96a2-6c28dae02bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01611d69-048f-4c26-ba69-4e3609c02acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caade9-814b-4b32-b94d-e9bc8a7cf83b",
   "metadata": {},
   "source": [
    "## Split into Train/Validation Datasets\n",
    "\n",
    "90/10 split. This will help us understand how much our model is overfitting by hiding 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7ff0a3-7cab-49cb-aee3-4b6b8ed9ffe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))  # Determine cut point of 90%\n",
    "train_data = data[:n]  # First 90% is train\n",
    "val_data = data[n:]  # Last 10% is validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25c21-14a8-4117-8997-363612603895",
   "metadata": {},
   "source": [
    "## Concept of Block Size\n",
    "\n",
    "Feeding everything into the model would be computationally expensive and prohibitive. So we are going to separate it out into chunks of 8. But because we're interested not just in the character but the relationship from character to character, we're going to look at `block_size + 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c8c674-5406-4268-91dc-808423b49432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54ea4f-f7e5-4682-b848-735fb0426e74",
   "metadata": {},
   "source": [
    "Illustrating this with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538b23b3-2f4b-4049-8aeb-775b16db4563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af06a2-72d5-4e1d-92ca-79e0db3e1c2f",
   "metadata": {},
   "source": [
    "## Concept of Minibatches\n",
    "\n",
    "For effeciency we're going to batch multiple blocks together in batches. GPUs are really good at parellelizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b62906c4-759a-49ba-85d5-25ededf9c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4  # How many independent sequences will we process in parallel?\n",
    "block_size = 8  # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957c72ed-442c-421b-8dd0-65c0480a8934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Inputs ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "== Targets ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "========================\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('== Inputs ==')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('== Targets ==')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('========================')\n",
    "for b in range(batch_size):  # Batch dimension\n",
    "    for t in range(block_size):  # Block dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00298-ad41-426a-a51a-ba1375d7780b",
   "metadata": {},
   "source": [
    "## Feed into a Neural Network\n",
    "\n",
    "We'll start with the simplest type of NN, a bigram languge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d864d3f-903c-4d34-9271-b792fc8eb85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape tensors for use in cross_entropy function\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Logits needs to be 2 dimensional\n",
    "            targets = targets.view(B * T)  # Reshape targets to 1 dimensional\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indeces in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(1000)\n",
    "        for k in range(1000):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7ec4714-c131-4ee4-8c74-b022b84d6fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45f5ca-e7fa-4981-8459-13f9112f070b",
   "metadata": {},
   "source": [
    "Our loss is stated in the `loss` tensor. This will change at each run but when I wrote this it was 5.0364. We can calculate the expected loss as the negative natural logarithm of 1 over the number of dimensions. In our case that is $-{\\ln}\\frac{1}{65} = 4.174$. The fact that our loss is higher means our model is too diffuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cb0e1-3ec7-4470-984f-ca267f19d336",
   "metadata": {},
   "source": [
    "## Generate Predictions \n",
    "\n",
    "I've now added a `generate` method to the `BigramLanguageModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57638cc-1ed1-4033-a946-122faa92d190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1x1 tensor holding a zero to kick off the generation\n",
    "# Remember a 0 corresponds to a newline (\\n) character\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce849938-047b-4f51-9e0c-b593c1c6feb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd891371-e27c-430b-9a1e-11bf85df888e",
   "metadata": {},
   "source": [
    "## WHAT!? This is Garbage!\n",
    "\n",
    "It's garbage because this is a randomly initialized model. It hasn't been trained at all. Now let's actually train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3334311c-46c3-4cf8-ae0c-729243ff07c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pytorch optimization object using AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2edc8-efe3-4d60-a1c3-b308aa11f96e",
   "metadata": {},
   "source": [
    "The optimizer we just created will take the gradients and update the parameters using the gradients. Another possible optimizer would be Stochastic Gradient Descent (torch.optim.SDG).\n",
    "\n",
    "Now let's set it all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092230ed-03a5-4389-8f9f-ee017a534034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6367, val loss 4.6454\n",
      "step 1000: train loss 3.6943, val loss 3.7064\n",
      "step 2000: train loss 3.1163, val loss 3.1335\n",
      "step 3000: train loss 2.8047, val loss 2.8225\n",
      "step 4000: train loss 2.6405, val loss 2.6581\n",
      "step 5000: train loss 2.5641, val loss 2.5805\n",
      "step 6000: train loss 2.5160, val loss 2.5428\n",
      "step 7000: train loss 2.4933, val loss 2.5190\n",
      "step 8000: train loss 2.4839, val loss 2.5065\n",
      "step 9000: train loss 2.4701, val loss 2.4969\n",
      "2.401021718978882\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10000):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if steps % 1000 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf479d-f7b8-4161-9cd1-9caafbb51989",
   "metadata": {},
   "source": [
    "Our loss is now greatly reduced. Now lets get predictions again. It will look a little better, but it's not shakespeare yet! Not even readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41f92dd-ab67-4a7c-b8ac-b7f5d35a85c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tamby,\n",
      "Say d, ango thinouggof d RLUTous ds.\n",
      "O:\n",
      "NNoucoferesainteiss,\n",
      "ENCh;\n",
      "Heme LEEN:\n",
      "RYConsprear tiop!\n",
      "If k arony ob's arin seler e 'sulisthet is. maitr:\n",
      "Mou e, th se ent arare dow inencary;\n",
      "LerX?\n",
      "Ascut w d Dime d tho,\n",
      "SThtaive b,\n",
      "Twhe ho.\n",
      "AYe t y mu d tha'd\n",
      "Thayofe st sos sowin himy itore pas bethe\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068a28b-d2cf-4ef4-b7e1-b7e0cafdb526",
   "metadata": {},
   "source": [
    "# Next... Increase complexity of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a312b69-8ae9-46a3-b5ff-254a81fd5bdf",
   "metadata": {},
   "source": [
    "Our model up to this point has been incredibly simple, the tokens aren't talking to each other. We're only using the last character to figure out what comes next. Now we're going to get these tokens talking to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d8027-bfe8-4c3b-bb95-8d346734a67c",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0b14105-cd88-4951-87e2-ee72c54e9ae9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa7b83-3aa8-4d3b-8408-781bf5cd98aa",
   "metadata": {},
   "source": [
    "Currenty the 8 tokens in a batch (aka time) are not talking to each other. We want them to talk to each other. But we only want tokens talking to the token that comes before them. AKA, token in position 5 can't see tokens 6, 7, 8 but can see tokens 1-4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a1793-3ae1-4ab7-81d7-97b4c1310307",
   "metadata": {},
   "source": [
    "### Version 1: For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78219ada-6b3d-4108-81fe-53653c8da3ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# bow = bag of words, there's a word stored at every one of the T positions\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    # print('======')\n",
    "    # print(b)\n",
    "    for t in range(T):\n",
    "        # print(f'  ->{t}')\n",
    "        xprev = x[b, :t+1]\n",
    "        # print(f'    {xprev}')\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e912c45-b0a8-4b00-8662-306eeab7921c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0894"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.1808 + -0.3596) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5f6c4-7dbd-41f8-b8c8-c9fbdb5c0d4b",
   "metadata": {},
   "source": [
    "#### Look at the output!\n",
    "Now it starts to make sense. Let's start by comparing `x[0]` to `xbow[0]`. The first line is equal, but the second line is now the average of the two because $(0.1808 + -0.3596) / 2 = -0.0894$ and $(-0.0700 + -0.9152) / 2 = -0.4926$. Then the third position is the average of the three, and so on. The last element is the vertical average of all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2658e4d2-97a1-4ff8-84e4-cf7e0106daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98046053-b50f-4266-8c6d-1e9e155a2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3e627-7992-4027-aec7-8b0201a289ba",
   "metadata": {},
   "source": [
    "### Version 2: Matrix Multiplication\n",
    "This is great BUT for loops are inefficient. Would be much better to do with Matrix multiplication. Lets look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "618d6540-6b57-45f0-9b16-34715f7d08ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.ones(3, 3)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9109-4609-4f20-a2d3-adbcd86b980f",
   "metadata": {},
   "source": [
    "The dot product in position 1 of tensor `c` (value of 14) comes by multiplying the first **row** of tensor `a` by the first **column** of tensor `b`. Since `a` is all ones it ends up just being a sum of `a` column 1.\n",
    "\n",
    "But `torch` has another great tool called `torch.tril()` that gives just the lower triangular values of a tensor and zeroes out the rest. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acb09821-2541-4efe-9555-57ddc1f0556b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones, wrap in tril to zero out the top right and give running sums\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7b1a6-ec20-428a-969a-ef79a164d4e8",
   "metadata": {},
   "source": [
    "When we do this we end up with a **running sum**! To make them averages only takes a little more effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c720f549-0ee3-497f-a46b-4c0864bf5b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a94c42-851f-4da7-ac30-53c22cac99ea",
   "metadata": {},
   "source": [
    "Now we can vectorize this and scale to our previous example with `x` and `xbow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1205d56e-3596-4688-bba2-c293449b74fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8b5c4-e17e-41fb-a835-b698bdd86bbf",
   "metadata": {},
   "source": [
    "### Version 3: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f578d7-cff4-4aad-a503-236d25d60613",
   "metadata": {},
   "source": [
    "Softmax is a noramlization operation, so we get the same matrix. The reason we want to use *this one* is because the weights will start with zero but eventually the relationships will not be constant at zero but will be data dependent. We'll call these affiinities, some values find others more interesting and that is the basis of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "375c47eb-b40d-4038-9bd7-e9d09b2f334a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix and set aside\n",
    "wei = torch.zeros((T, T))  # Initialize wei as all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is how we keep the past from seeing the future\n",
    "\n",
    "# Doing a softmax along every row (because dim = -1), softmax is a regularization\n",
    "# function so it does the same as in version 2.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574678b1-59fd-4dfc-b2a4-4987c3264d92",
   "metadata": {},
   "source": [
    "### Version 4: Self-Attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79d39800-fee5-42a1-9560-6256c4e38fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # Now C is 32\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fc861-4185-4f8c-ad07-9d5d142d2db4",
   "metadata": {},
   "source": [
    "Previously we initiliazed `wei` with all zeros. We need to change this, because we don't want all zeros because some tokens will have more/less affinity for others. If we initialize as zero then wei has uniform rows, but that's not what we want in real life. Some tokens will find other tokens more/less interseting and we want this to be data-dependent.  Ex., a vowel might be looking for consanants in past, we want to know what they are and flow to the next token.\n",
    "\n",
    "How we solve: Every token at each position will emit TWO VECTORS. A query and a key vector: What am I looking for? Key vector: What do I contain. The affinities come by doing a dot product with keys and queries. **That dot product becomes `wei`.** If key/query are aligned, they interact to a higher degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36c5cd-953b-4f06-a72c-fc73da9818f5",
   "metadata": {},
   "source": [
    "#### Let's see a single head perform self-attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14fb0b04-b261-4903-83e7-f645febae4db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0243,  0.7806,  0.3592,  0.2484,  0.2203, -0.2434,  0.5941,  1.2976,\n",
      "          0.4907, -0.9231, -0.7141,  0.8663,  0.5429, -0.4535, -0.5635, -0.2417],\n",
      "        [-0.0669,  0.8692, -0.0702,  0.5468,  1.0354, -1.2248, -0.0357, -0.3485,\n",
      "         -0.3257,  0.8254,  0.6185,  1.2044,  0.7290, -0.6172, -0.1570, -0.2020],\n",
      "        [-0.1794, -0.5119, -0.6379, -0.2220,  0.7300, -0.0094, -0.3451,  1.1641,\n",
      "         -0.2302, -0.1707,  0.2256,  0.2680,  0.8527, -0.4898,  0.6744,  0.1902],\n",
      "        [-0.5010,  0.0236, -0.3745,  0.2033,  0.3464, -0.5504, -0.2041, -0.0295,\n",
      "         -0.4679,  1.0379,  0.3958,  0.1890,  0.5071,  0.5054,  0.7219, -0.6512],\n",
      "        [ 0.2949, -0.3837, -0.3887, -0.4149, -0.2714, -0.2772, -0.1827,  0.6626,\n",
      "          0.7409, -0.6932, -0.0868, -0.8386,  0.5869,  0.3283, -0.7405,  0.5118],\n",
      "        [-0.0936,  0.6212,  0.5187,  1.2611, -0.8386, -0.1025, -0.3600,  0.1297,\n",
      "         -0.3774, -0.6213, -0.0153,  0.2046,  0.1706, -1.3380, -0.7898, -0.7704],\n",
      "        [ 0.0977,  1.0481,  0.0323,  0.3505, -0.4278, -0.1419, -0.5668, -0.5803,\n",
      "         -0.2357,  0.2426,  0.3279, -0.0708,  0.2094, -0.0269, -0.2700,  0.7200],\n",
      "        [ 0.0025, -0.2965,  0.1499,  0.4601,  0.1558,  0.3118, -0.8677, -0.5294,\n",
      "         -0.5124, -0.2328,  0.2359, -0.0438, -0.5761, -0.2079,  0.2228,  0.2678]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0988,  0.1457,  0.2281, -0.6538,  0.0510, -0.2049,  0.3042, -0.8043,\n",
      "         -0.4855,  0.4098, -0.0398,  0.0063, -0.2967, -0.3055, -0.2479,  0.8227],\n",
      "        [-0.5430,  0.0597,  0.0851, -0.3093,  0.5240, -0.5309,  0.4679, -0.2784,\n",
      "          0.9924,  0.1222, -0.8497, -0.6268, -0.7090, -0.6624,  0.7858, -0.4354],\n",
      "        [-0.3917,  0.0195, -0.4033,  0.7731,  0.1353, -0.8510,  0.2174,  0.0845,\n",
      "          0.5319, -0.5876,  0.3215, -0.0380, -0.7103,  0.5052,  0.2627,  0.8951],\n",
      "        [ 0.0184, -0.1507,  0.3064,  1.2997, -0.1544, -0.3029,  0.0960,  0.7646,\n",
      "          0.0834, -0.5875, -0.0499,  0.3666,  0.9641,  0.1954,  0.0165,  0.0080],\n",
      "        [-0.4221, -0.8528, -0.6090,  0.8359,  0.1355, -0.5007, -0.2075, -0.0356,\n",
      "          0.6374,  0.4974,  0.2593,  0.7119, -0.7412,  0.6236, -0.5676,  0.3706],\n",
      "        [-0.3277,  0.3650,  0.2171,  0.4074, -0.2705, -0.2293, -0.8352,  0.1896,\n",
      "          1.0591, -0.5025, -0.3055,  0.7403,  0.2529, -0.1694,  0.5670, -0.2637],\n",
      "        [-1.2210,  0.3288, -0.3218, -0.2177,  0.0514, -1.0945,  0.1910,  0.1796,\n",
      "          0.3625, -0.2933, -0.7292,  0.4105, -0.2294,  0.1467,  0.0503,  0.0612],\n",
      "        [-0.2384, -0.2416, -0.2999,  0.0968, -0.3263,  0.7460,  0.1059,  0.2051,\n",
      "          0.2219,  0.4294, -0.3218,  0.4395,  0.4151,  0.6831,  0.3031, -0.3836]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 16  # Hyperparameter for head size\n",
    "key = nn.Linear(C, head_size, bias=False)  # Initialize linear modules with bias=False to avoid getting fixed weights\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Produce k and q by forwarding key and query on x\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "print(k[0])\n",
    "print(q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27956bd-7cdf-4583-911a-e9ba3b2bd9eb",
   "metadata": {},
   "source": [
    "When we forwarded `key` and `query` on `x`. Each token in all positions in BxT arrangement produce a key and query. No communication has happened yet. That happens now. All the queries will dot product with each of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51205fea-4085-49cd-83a6-43a12250334e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need to transpose to the right dimension before dot product. What we get is:\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45cf55d8-97d7-4159-911e-349c6c294d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now add the other steps in to remove the top triangle of data (otherwise we're leaking data)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix and set aside\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is how we keep the past from seeing the future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f94f27a3-443b-4398-9b5c-140facb625c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6513, 0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3639, 0.3610, 0.2751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6837, 0.1142, 0.1494, 0.0528, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0632, 0.3618, 0.1033, 0.3048, 0.1669, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4138, 0.1132, 0.0976, 0.0557, 0.0559, 0.2638, 0.0000, 0.0000],\n",
       "        [0.4149, 0.1918, 0.0849, 0.1211, 0.0862, 0.0532, 0.0479, 0.0000],\n",
       "        [0.1260, 0.0522, 0.1892, 0.3630, 0.0945, 0.0475, 0.0607, 0.0670]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For every row of B we'll have a T^2 matrix giving us the weight.\n",
    "# The weights are no longer uniform like they were before but indicate\n",
    "# affinities between tokens.\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a9b3f-cbdc-41ed-a332-110b58b24722",
   "metadata": {},
   "source": [
    "## NB -> Script Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3894e55-8d47-44a9-aa89-db16605a2d9d",
   "metadata": {},
   "source": [
    "This will be easier if we work from a script instead of a notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
