{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f1b353-9a48-4e2c-a13d-6a0d90a38592",
   "metadata": {},
   "source": [
    "# Train a Transformer to Write Shakespeare\n",
    "\n",
    "This tutorial notebook is based on [this video](https://www.youtube.com/watch?v=kCc8FmEb1nYhttps://www.youtube.com/watch?v=kCc8FmEb1nY)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a9a0d-4ad7-4c9a-ac7e-031b56f08e2b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8252e7ec-d7bb-4ce9-90a2-97383460c6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a2373-2d45-47a0-8be7-2566b1717d45",
   "metadata": {},
   "source": [
    "## Download and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e57f241-deda-4e5b-8e88-02bd9fc25215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebbf299-6b88-4481-b666-2edf121ca45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d67c5d7-36a6-46c8-a2b0-86eaa4b071c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset in characters: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33af1dc-1a4a-474f-98b4-adb123214554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first 500 characters\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db8528f-3d2d-4fc7-ba08-305c0844cd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters in the text:\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'There are {vocab_size} unique characters in the text:')\n",
    "print(''.join(chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c400c7-1132-4bf3-af69-f9f55360d431",
   "metadata": {},
   "source": [
    "## Create a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b9fd0-c315-4a88-8c11-9c3b012b177a",
   "metadata": {},
   "source": [
    "Tokenizing is to put the text in a language the computer can understand, in this case a list of characters. We're doing something really simple, just assigning an integer to each of the possible 65 characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe4923-9744-4b5b-b7e9-fe0eec1d6e73",
   "metadata": {},
   "source": [
    "First we build the mapping ourselves by creating a lookup table in both directions, string to integer (`stoi`) and vice versa (`itos`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6565984d-317f-4967-a19e-83e1fe4664df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# String to Integer Mapping\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Integer to String Mapping\n",
    "itos = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd0e20-cf51-44dc-8a8a-c64868c78fc7",
   "metadata": {},
   "source": [
    "Now write two quick functions, one to encode (string to int) and one to decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4040eec-426f-43a6-9fc4-03d5fef70fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create quick functions using lambda (lambda is used for one-liners like this)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111c211-b451-4379-a6b9-6767773437d9",
   "metadata": {},
   "source": [
    "Now test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e0bd48-8fc8-42a7-955e-c12930db5456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hii there!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there!\"))\n",
    "print(decode(encode(\"hii there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9254e-64a7-48db-b8ea-10224f82c72f",
   "metadata": {},
   "source": [
    "And test on the Shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa7fd07-65b2-4f86-9b54-158da09f9d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test encoding on first 15 characters of text\n",
    "encoded_txt = encode(text[:15])\n",
    "encoded_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ec86e1-e65e-4c3e-8391-8d9bf6a52d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now decode it\n",
    "decode(encoded_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419546f5-bf9c-41da-b56b-7fcda8fefc49",
   "metadata": {},
   "source": [
    "There are many different ways to build a tokenizer, we just did a very simple one. For example, Google created one called [SentencePiece](https://github.com/google/sentencepiece). It tokenizes at the sub-word level, which is usually adopted in practice for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2a805-c553-414d-97d2-ed586eb2836d",
   "metadata": {},
   "source": [
    "## Tokenize the Text\n",
    "\n",
    "We're going use our basic encoder to encode the entire text and store it into a `torch.Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7359bc35-919a-4ade-96a2-6c28dae02bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01611d69-048f-4c26-ba69-4e3609c02acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caade9-814b-4b32-b94d-e9bc8a7cf83b",
   "metadata": {},
   "source": [
    "## Split into Train/Validation Datasets\n",
    "\n",
    "90/10 split. This will help us understand how much our model is overfitting by hiding 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7ff0a3-7cab-49cb-aee3-4b6b8ed9ffe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))  # Determine cut point of 90%\n",
    "train_data = data[:n]  # First 90% is train\n",
    "val_data = data[n:]  # Last 10% is validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25c21-14a8-4117-8997-363612603895",
   "metadata": {},
   "source": [
    "## Concept of Block Size\n",
    "\n",
    "Feeding everything into the model would be computationally expensive and prohibitive. So we are going to separate it out into chunks of 8. But because we're interested not just in the character but the relationship from character to character, we're going to look at `block_size + 1`. This way we can go to up to 8 characters and see the following character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c8c674-5406-4268-91dc-808423b49432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54ea4f-f7e5-4682-b848-735fb0426e74",
   "metadata": {},
   "source": [
    "There are 8 examples of data embedded here. There's the first (18), which is followed by 47. There's the first AND second (18, 47) and followed by 56. We're going to refer to this dimension as the **time dimension**. Illustrating this with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538b23b3-2f4b-4049-8aeb-775b16db4563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af06a2-72d5-4e1d-92ca-79e0db3e1c2f",
   "metadata": {},
   "source": [
    "## Concept of Minibatches\n",
    "\n",
    "For effeciency we're going to batch multiple blocks together in batches (also called *minibatches*). They are processed separately and don't talk to each other. GPUs are really good at parellelizing. This is the **batch dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b62906c4-759a-49ba-85d5-25ededf9c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)  # Now we're setting a random seed because we're grabbing random samples in the data\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4  # B: How many independent sequences will we process in parallel?\n",
    "block_size = 8  # T: What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # create 4 random starting places in the text\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "957c72ed-442c-421b-8dd0-65c0480a8934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Inputs ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "== Targets ==\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "========================\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('== Inputs ==')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('== Targets ==')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('========================')\n",
    "for b in range(batch_size):  # Batch dimension\n",
    "    for t in range(block_size):  # Block dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00298-ad41-426a-a51a-ba1375d7780b",
   "metadata": {},
   "source": [
    "## Feed into a Neural Network\n",
    "\n",
    "Now we know what kind of input we want to feed to our transformer, it's an 8x4 tensor. We'll start with the simplest type of NN, a bigram languge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455dd0d0-92b6-4218-830a-23fa48fb5033",
   "metadata": {},
   "source": [
    "### Simplest Bigram Language Model\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1331s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61372162-958a-4fff-9fee-62025dfb89cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f330652-7301-4067-92dd-2adb66a7e539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a6ea4-8915-4ee5-bdd8-5ee14faa0e29",
   "metadata": {},
   "source": [
    "So what is this doing? We're feeding our minibatch into it. See previous section to remind what `xb` and `yb` are. The embedding table is 65x65. As the `forward` method runs, every integer is going to pluck a row of the embedding table corresponding to its index. Then Pytorch arranges it as a *Batch x Time x Channel* tensor. So we have a 65-length tensor for every one of the 4 x 8 positions in our input set. \n",
    "\n",
    "The output are the `logits` or the scores for the next characters in the sequence. We're predicting what comes next based *only* on the location of the token in the sequence. Ie what are the odds of each character coming up in position 5 in the 1st minibatch?\n",
    "\n",
    "We can figure that out!\n",
    "\n",
    "First, use `torch.sort()` to sort the embedding table of the fifth position of the first minibatch (remember zero-indexing). Then recall the top 5 indices. Looks like our most likely character is 22. We can remind ourselves which character is index 22 in our vocab list. This really isn't going to be very accurate, we're not measuring loss, we're not optimizing anything. It's about as good as a random guess, as we'll see soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2b3344-de11-47f8-8cb1-5e2ac30d0e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1471, 1.9577, 1.3954, 1.1822, 1.1564], grad_fn=<SliceBackward0>)\n",
      "tensor([10, 55, 44, 64,  4])\n",
      "Index 10 is: \":\"!\n"
     ]
    }
   ],
   "source": [
    "sorted, indices = torch.sort(out[0][4], descending=True)\n",
    "\n",
    "print(sorted[:5])\n",
    "print(indices[:5])\n",
    "print(f'Index {indices[0]} is: \"{chars[indices[0]]}\"!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45dee8-b2db-4b0c-ac48-9db3467844c7",
   "metadata": {},
   "source": [
    "### Add Loss\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1500s)\n",
    "\n",
    "Now that we've made predictions about what comes next, let's evaluate them with a **loss function**. A good way to measure the loss, or quality of the prediction is to use *negative log likelihood loss*, which is implemented in PyTorch under the function `cross_entropy()`. Loss is the cross entropy of the predictions and the targets. We measure the quality of the logits with respect to the targets. We have the identity of the next character, so we want to know how well we're predicting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f620d7f-5ccb-4ad5-adf5-198785ed873d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        # Loss is cross entropy, we have to reshape the input though per documentation\n",
    "        # It wants B x C x T rather than B x T x C, so we'll reshap logits\n",
    "        B, T, C = logits.shape # unpack\n",
    "        logits = logits.view(B*T, C)  # Stretch them out into 1-dim sequence, C is 2nd dim\n",
    "        \n",
    "        # Also need to do same to targets\n",
    "        targets = targets.view(B*T)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bebc8f1-5a29-43be-a118-8a9a65205793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c3d4d-5f06-4f24-8ce2-59e2e616da18",
   "metadata": {
    "tags": []
   },
   "source": [
    "It should be obvious now how we \"stretched\" logits to make it work for the cross_entropy function. Instead of 4 x 8 x 65 we have 32 x 65. Let's look at the first one, which would have corresponded to the first index of the first minibatch. It's a 65-len tensor giving all my logits. I can again sort it to figure out the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "085a84de-81a7-42b3-baf8-380de91c812d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4849, -0.8661, -1.0219, -1.1038, -0.3763, -0.2251, -0.2825,  0.4622,\n",
      "        -0.7136, -1.4831,  1.3023, -1.2810,  1.2857,  0.8153, -1.4935, -0.1127,\n",
      "         0.7719,  2.9956,  0.2084, -1.6306,  1.4533, -1.1483,  0.7007,  1.2882,\n",
      "         0.7806,  1.2904,  0.0471,  1.4801, -0.6316, -1.1766, -1.5717,  0.5684,\n",
      "         1.2815,  0.4047,  0.0632,  0.5846, -0.1738,  0.8185, -0.5315, -0.7415,\n",
      "         0.6128,  0.9535, -0.0584, -0.4370,  0.2026, -0.8318, -0.1020,  0.9157,\n",
      "        -0.6446, -0.5180,  0.8405, -1.3159,  0.0663, -0.7541,  0.7109, -0.3921,\n",
      "        -1.4153, -0.0123,  0.2143,  1.5742, -1.7377,  0.9368,  0.1410,  1.5414,\n",
      "         0.7376], grad_fn=<SelectBackward0>)\n",
      "tensor([2.9956, 1.5742, 1.5414, 1.4849, 1.4801], grad_fn=<SliceBackward0>)\n",
      "tensor([17, 59, 63,  0, 27])\n",
      "Index 17 is: \"E\"!\n"
     ]
    }
   ],
   "source": [
    "print(logits[0])\n",
    "\n",
    "# Sort\n",
    "sorted, indices = torch.sort(logits[0], descending=True)\n",
    "\n",
    "# Output Results\n",
    "print(sorted[:5])\n",
    "print(indices[:5])\n",
    "print(f'Index {indices[0]} is: \"{chars[indices[0]]}\"!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989b6a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7288, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725bd10",
   "metadata": {},
   "source": [
    "Our loss is stated in the `loss` tensor, output in the previous cell. This will change at each run but when I wrote this it was 4.7288. We can calculate the expected loss as the negative natural logarithm (hey look, NEGATIVE LOG LIKELIHOOD!?) of 1 over the number of dimensions. In our case that is $-{\\ln}\\frac{1}{65} = 4.174$. The fact that our loss is higher than expected means our model is NOT diffuse. We have entropy, we're guessing wrong. We want much lower loss than the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582c217",
   "metadata": {},
   "source": [
    "### Add Generation\n",
    "\n",
    "[Video Chapter](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=28m50s)\n",
    "\n",
    "Still, despite our garbage model, we're still going to try some **generation**! Lets add that to our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8aae16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a\n",
    "        # lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (BxTxC dimensions or 4x8x65)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss is cross entropy, we have to reshape the input though per documentation\n",
    "            # It wants B x C x T rather than B x T x C, so we'll reshap logits\n",
    "            B, T, C = logits.shape # unpack\n",
    "            logits = logits.view(B*T, C)  # Stretch them out into 1-dim sequence, C is 2nd dim\n",
    "\n",
    "            # Also need to do same to targets\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indeces in the current context. The job of generate is to \n",
    "        # take the B x T array and extend it to be B x T +1, +2, +3, etc. up until\n",
    "        # max_new_tokens is reached.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # For now we ignore loss, we're only using logits\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14168eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.2793, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # Feed a minibatch into it\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79736d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1x1 tensor holding a zero to kick off the generation\n",
    "# Remember a 0 corresponds to a newline (\\n) character\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "736279d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p\n",
      "R:?!T,OAjIZqMheATXkO Y,kMzT'wpgwkxzUltuG-:X;PRyM:Ryu?eU\n",
      "E'yrsx'KfkCO$Dx'3UM..FB:RTaQjd\n",
      "lhF.AHrq!ms\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0a638",
   "metadata": {},
   "source": [
    "### WHAT!? This is Garbage!\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2093s)\n",
    "\n",
    "It's garbage because this model was never trained, not at all. Now let's actually train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "435cf0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pytorch optimization object using AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3edaa3",
   "metadata": {},
   "source": [
    "The optimizer we just created will take the gradients and update the parameters using the gradients. Another possible optimizer would be Stochastic Gradient Descent (`torch.optim.SDG`).\n",
    "\n",
    "Now let's set it all up and run with just 10 iterations and see how much loss we get. Now we're actually training the model, we're feeding the data into our model and updating parameters using our `optimizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9897a4c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45624303817749\n",
      "4.506674766540527\n",
      "4.555857181549072\n",
      "4.562231063842773\n",
      "4.483057975769043\n",
      "4.409609794616699\n",
      "4.45909309387207\n",
      "4.423033714294434\n",
      "4.445444107055664\n",
      "4.436178207397461\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e276ab9",
   "metadata": {},
   "source": [
    "It's getting lower, but lets train more next. We'll do 10k iterations, and only print loss at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d78971d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.3499817848205566\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "print(f'final loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1637cf4",
   "metadata": {},
   "source": [
    "Our loss is now greatly reduced. Now lets get predictions again. It will look a little better, but it's not shakespeare yet! Not even readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "682af826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAROnonghisil aun,\n",
      "tou, y.\n",
      "\n",
      "\n",
      "tthe; re, FQ\n",
      "I:\n",
      "\n",
      "DUES:\n",
      "Wh,'torr ssentonknthes; terimempist!\n",
      "TDespowZARD3ewo; barehe:\n",
      "O\n",
      "C mart,\n",
      "ARDot Thand ingie, ofongren.\n",
      "WARTCo blevengarof c!\n",
      "Cl,-sof wom'erd ICpp!'r thandunsepl?\n",
      "TYiAndeamy y meand lind ar t ge l ME:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be3f1",
   "metadata": {},
   "source": [
    "We could even train 10k more times and see if that helps, probably not though, because we've likely reached some kind of maximum given the simplicity of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b39ea42a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 2.2936747074127197\n",
      "\n",
      "I hengord te I whamett pis by cer nathopyeareenop:\n",
      "Ly m tome piand.\n",
      "\n",
      "CAr ICENGRTant:\n",
      "Thon ator:\n",
      "Hepat br ble tid nonengelle t G ne e stos,\n",
      "THAnd boune And s ooreth sulf ane hy y woford'shoulellon, medeifents,\n",
      "t mutose ca?\n",
      "\n",
      "INGes th my bl t ghe hyot I\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # Use a bigger batch size now\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data from train\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss using a typical training loop\n",
    "    logits, loss = m(xb, yb)  # Evalueate loss\n",
    "    optimizer.zero_grad(set_to_none=True)  # Zero gradeints from prev ste\n",
    "    loss.backward()  # Get gradients for all params\n",
    "    optimizer.step()  # Use gradients to update parameters\n",
    "\n",
    "print(f'final loss: {loss.item()}')\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fe701",
   "metadata": {},
   "source": [
    "Yeah... training this over and over isn't helping, there's just only so much such simple architecture can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068a28b-d2cf-4ef4-b7e1-b7e0cafdb526",
   "metadata": {},
   "source": [
    "# Next... Increase complexity of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a312b69-8ae9-46a3-b5ff-254a81fd5bdf",
   "metadata": {},
   "source": [
    "Our model up to this point has been incredibly simple, the tokens aren't talking to each other. Though we're training on blocks of up to 8, we're only using the last character to figure out what comes next. Now we're going to get these tokens talking to each other in a more optimized way! That's where self-attention comes in.\n",
    "\n",
    "**NOTE:** The implementation of self-attention will happen in `bigram.py`, but the rest of this notebook will be used to illustrate self-attention prior to implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d8027-bfe8-4c3b-bb95-8d346734a67c",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention\n",
    "\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2533s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0b14105-cd88-4951-87e2-ee72c54e9ae9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consider the following toy example with random data in a 4x8x2 array:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa7b83-3aa8-4d3b-8408-781bf5cd98aa",
   "metadata": {},
   "source": [
    "Currenty the 8 tokens in a batch (aka time) are not talking to each other. We want them to talk to each other. But we only want tokens talking to the token that comes before them. AKA, token in position 5 can't see tokens 6, 7, 8 but can see tokens 1-4. We want to do a rolling average but not allow tokens to look into the future. A rolling average isn't great, we lose information but that's fine for this example.\n",
    "\n",
    "So for every *nth* step, we want to capture avereage of the *nth* and previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a1793-3ae1-4ab7-81d7-97b4c1310307",
   "metadata": {},
   "source": [
    "### Version 1: For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78219ada-6b3d-4108-81fe-53653c8da3ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# bow = bag of words, there's a word stored at every one of the T positions\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    # print('======')\n",
    "    # print(b)\n",
    "    for t in range(T):\n",
    "        # print(f'  ->{t}')\n",
    "        xprev = x[b, :t+1]\n",
    "        # print(f'    {xprev}')\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "print(x.shape)\n",
    "print(xbow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5f6c4-7dbd-41f8-b8c8-c9fbdb5c0d4b",
   "metadata": {},
   "source": [
    "#### Look at the output!\n",
    "Now it starts to make sense. Let's start by comparing `x[0]` to `xbow[0]`. The first line is equal because it's just an average of itself, but the second line is now the average of the two because $(0.1808 + -0.3596) / 2 = -0.0894$ and $(-0.0700 + -0.9152) / 2 = -0.4926$. Then the third position is the average of the three, and so on. The last element is the vertical average of all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2658e4d2-97a1-4ff8-84e4-cf7e0106daaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98046053-b50f-4266-8c6d-1e9e155a2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3e627-7992-4027-aec7-8b0201a289ba",
   "metadata": {},
   "source": [
    "### Version 2: Matrix Multiplication\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2831s)\n",
    "\n",
    "This is great BUT for loops are inefficient. Would be much better to do with Matrix multiplication. Lets look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "618d6540-6b57-45f0-9b16-34715f7d08ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.ones(3, 3)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9109-4609-4f20-a2d3-adbcd86b980f",
   "metadata": {},
   "source": [
    "The **dot product** in position 1 of tensor `c` (value of 14) comes by multiplying the first **row** of tensor `a` by the first **column** of tensor `b`. Since `a` is all ones it ends up just being a sum of `a` column 1. See here for [dot product refresher](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-matrices/a/multiplying-matrices).\n",
    "\n",
    "But `torch` has another great tool called `torch.tril()` that gives just the lower triangular values of a tensor and zeroes out the rest. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acb09821-2541-4efe-9555-57ddc1f0556b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones, wrap in tril to zero out the top right and give running sums\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7b1a6-ec20-428a-969a-ef79a164d4e8",
   "metadata": {},
   "source": [
    "When we do this we end up with a **running sum**! To make them averages only takes a little more effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c720f549-0ee3-497f-a46b-4c0864bf5b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a 3x3 tensor of all ones\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "# Create a random 3x2 tensor \n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "# Compute dot product of two matrices\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a94c42-851f-4da7-ac30-53c22cac99ea",
   "metadata": {},
   "source": [
    "Now we can vectorize this and scale to our previous example with `x` and `xbow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1205d56e-3596-4688-bba2-c293449b74fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8b5c4-e17e-41fb-a835-b698bdd86bbf",
   "metadata": {},
   "source": [
    "### Version 3: Softmax\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3282s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f578d7-cff4-4aad-a503-236d25d60613",
   "metadata": {},
   "source": [
    "Softmax is a normalization operation, so we get the same matrix. The reason we want to use *this one* is because the weights will start with zero but eventually the relationships will not be constant at zero but will be data dependent. Now we'll start to call these **affiinities**, some values find others more interesting and that is the **basis of attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c73f358-db55-4828-b496-dabd8c87ab00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it wei (short for weights)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix and set aside\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fba7dca-6082-47dc-9367-f94a0df72137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T, T))  # Initialize wei as all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is how we keep the past from seeing the future\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654082b-8ddb-481b-acc6-dddddfd83de0",
   "metadata": {},
   "source": [
    "If we do a softmax across every row of `wei`, what does it do? Softmax is a normalization operation, it *exponentiates* each element along a row ($e^x$), and then divides by the sums of those exponents. $e^0 = 1$ so we get 1 every time the `tril` tensor has a zero. Likewise $e^\\text{-Â inf} = 0$.\n",
    "\n",
    "With this in mind, we can calculate each row below by altering the `wei_idx` variable in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "310ea813-a0e5-4a6a-87b5-7d5e016bd266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 of wei is:                tensor([0., 0., -inf, -inf, -inf, -inf, -inf, -inf])\n",
      "Exponentiated that row becomes: tensor([1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "The sum of the exponents is:    2.0\n",
      "So softmax is:                  tensor([0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "wei_idx = 1\n",
    "\n",
    "print(f'Row {wei_idx} of wei is:                {wei[wei_idx]}')\n",
    "print(f'Exponentiated that row becomes: {torch.exp(wei[wei_idx])}')\n",
    "print(f'The sum of the exponents is:    {torch.sum(torch.exp(wei[wei_idx]))}')\n",
    "print(f'So softmax is:                  {torch.exp(wei[wei_idx]) / torch.sum(torch.exp(wei[wei_idx]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0e53e-13bd-45ce-971e-2ed47debfb16",
   "metadata": {},
   "source": [
    "Fortunately pytorch includes a softmax function, we just need to tell it along which dimension to calculate it and we can calculate softmax for each of our 8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1b90f51-394d-4528-80c8-5e5905ec1e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a softmax along every row (because dim = -1), softmax is a regularization\n",
    "# function so it does the same as in version 2.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596075a3-b1cf-412f-aafa-60bb95a38e62",
   "metadata": {},
   "source": [
    "And with that creatively devised mask in place, we can find the dot product and once again we get our nice rolling average.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ea5fa56-07c1-4dff-b3a9-0ff511581559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "# New version of xbow, to do all the Matrix multiplication\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bd0e9-cf7d-4d8e-b1ec-c1250883651c",
   "metadata": {},
   "source": [
    "#### Why do we care about softmax and all these different methods of doing a rolling average!?\n",
    "The reason we'll use this in self-attention (next), is because we will start changing the affinities. Up until now we've made them all zero (we initialized `wei` with `torch.zeros()`). Using softmax, we can do weighted self-attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574678b1-59fd-4dfc-b2a4-4987c3264d92",
   "metadata": {},
   "source": [
    "### Version 4: Self-Attention!\n",
    "#### Implement Small Self-Attention for a Single \"Head\"\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3720s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da415f15-6af3-42d5-acfa-decca8f6e734",
   "metadata": {},
   "source": [
    "We start off with what should be familiar. We've changed the number of channels from 2 to 32. So we have a 4x8 arrangement of tokens, and each token is 32-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79d39800-fee5-42a1-9560-6256c4e38fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # Now C is 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bb7a1-3a06-4711-8347-aa1a552efe12",
   "metadata": {},
   "source": [
    "Previously we initiliazed `wei` with all zeros. We need to change this, because we don't want all zeros because some tokens will have more/less affinity for others. So we're going to initialize them with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa5143e4-8986-4830-a558-40c3bcae28be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
       "         0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
       "         1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
       "         0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(B, T, C)\n",
    "x[0][0]  # Preview one group of C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bfb39-ab2b-47df-a3d9-232c92cedaee",
   "metadata": {},
   "source": [
    "Also, what we HAD before was a simple average of all the past tokens plus current token produced by our mask (`trill`). The past and current are being mixed together in an average. When we initialized the affinities as zero, then `wei` mask creates rows of uniform numbers, we don't want this (and this is why we moved to Softmax). Some tokens will find other tokens more/less interseting and we want this to be data-dependent.  Ex., a vowel might be looking for consanants in past, we want to know what they are and flow to the next token.\n",
    "\n",
    "How we solve: Every token at each position (we have 4 x 8 tokens or 32) will emit TWO VECTORS, a **query** and a **key** vector. \n",
    "\n",
    "- **Query Vector**: What am I looking for?\n",
    "- **Key Vector**: What do I contain?\n",
    "\n",
    "The **affinities** come by doing a dot product with key and query vectors. So my query dot product with all the keys of the other tokens. **That dot product becomes `wei`**. If key/query are aligned, they interact to a higher degree.\n",
    "\n",
    "*So let's implement that!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36c5cd-953b-4f06-a72c-fc73da9818f5",
   "metadata": {},
   "source": [
    "#### Let's see a single head perform self-attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efd861e9-4222-4939-ab0c-1b14bd09d0c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_size = 16  # Hyperparameter for head size\n",
    "\n",
    "# Set up key and query vectors\n",
    "key = nn.Linear(C, head_size, bias=False)  # Initialize linear modules with bias=False to avoid getting fixed weights\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb1adfff-71d5-4b96-aa8a-703402537be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=32, out_features=16, bias=False),\n",
       " Linear(in_features=32, out_features=16, bias=False))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1af8991b-a69d-4783-867d-f8f7d7889faf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Produce k and q by forwarding key and query on x\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27956bd-7cdf-4583-911a-e9ba3b2bd9eb",
   "metadata": {},
   "source": [
    "When we forwarded the linears `key` and `query` on `x` each token in all positions in B x T arrangement produce a key and query *independently*. No communication has happened yet. That happens now. All the queries will dot product with each of the keys.\n",
    "\n",
    "What we get that for every row of B, we have a T-squared matrix giving the affinities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b622409e-6ca6-415e-8f77-545e09c3de7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to transpose to the right dimension before dot product. What we get is:\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e3bd6b-9fae-4473-a518-9fd21434f901",
   "metadata": {},
   "source": [
    "These are the raw affinities between all the modes. There's a problem with what this though, we're leaking data because we haven't masked out the top triangle. We also need to normalize using Softmax. That will give us a nice distribution for each row that sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55ae12f2-f3ca-432f-84cb-d7334f8ef996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now add the other steps in to remove the top triangle of data (otherwise we're leaking data)\n",
    "tril = torch.tril(torch.ones(T, T))  # create same tril matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # replace zeros w/ -inf, this is again how we keep the past from seeing the future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf65da5-c892-47ed-8db5-3deb6c25002d",
   "metadata": {},
   "source": [
    "Now we have a masked, and normalized representation. But there's one more part to a single self-attention head. When we do the aggregation we don't aggregate the tokens exactly, we are going to set up one more vector called `value` just like we did for `key` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "021aa810-0b06-49af-b274-81c36a66b268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=32, out_features=16, bias=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dd76c-031e-44dc-840b-e460bf866bb6",
   "metadata": {},
   "source": [
    "We can think of `x` as being private to the token, which creates key and query. For the purpose of single-head attention `v` keeps the information that will be communicated between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "beafb097-8a73-4ab4-a3a7-314ac75d433c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x)  # Forward linear onto v\n",
    "out = wei @ v  # Dot product of wei on v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ef42-a2ca-4d7b-ba4e-1630023aa69b",
   "metadata": {},
   "source": [
    "## A Few Notes About Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ba04f-d959-4523-add5-f5c244b59eeb",
   "metadata": {},
   "source": [
    "![NODES](nodes.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a07822-b5f9-4bd0-a12e-ac241ac5ac8a",
   "metadata": {},
   "source": [
    "### Note 1: Attention is a Communication Mechanism\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4298s)\n",
    "\n",
    "We can think of attention as a communication mechanism. Think of it as a group of nodes in a directed graph like the one above, with edges pointing between nodes. Every node has a vector of information and aggregates information via a weighted sum from all the nodes that point to it. Our graph doesn't quite look like the above, because we have 8 nodes (block size is 8). The first node is only pointed to by itself, second is pointed to by itself and the first, all the way up to the eighth node that is pointed to by all previous nodes and itself. But in principle attention can be applied to any directed graph. It's just a communication mechanism between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667147fd-ac7b-403a-a8a8-b5891fc51813",
   "metadata": {},
   "source": [
    "### Note 2: Attention Has No Concept of Space\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4366s)\n",
    "\n",
    "Attention operates over a set of vectors in a graph. By default, nodes have no idea where they are in the space. This is why we needed to encode them positionally and give them information anchored to a specific position so they know where they are. This is different than a convolutional neural network, which has this concept out of the box. Here we had to specifically add it to the vectors it by masking (in our case we used `tril`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb53cfc-46b1-40b2-8fe3-b3a790118327",
   "metadata": {},
   "source": [
    "### Note 3: There is No Communication Across Batch Dimension\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4420s)\n",
    "\n",
    "Remember the minibatch section above? We separated into minibatches for the purpose of efficient computation. The batched dimensions never talk to each other. Because the batch size is four in our case, we could think of the problem as four separate graphs of 8 with their positional encoding. But there are no edges or links between those four independent batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae9981-3b19-480e-b309-ac7a6bc26256",
   "metadata": {},
   "source": [
    "### Note 4: Encoder Blocks vs Decoder Blocks\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4454s)\n",
    "\n",
    "Language models rely on directed graphs like the one we set up. Our structure is directional (I know we've talked about that over and over). Future tokens will not communicate with past tokens. But this doesn't have to be the case always for transformers. Sometimes nodes *can* talk to each other fully, like for example in sentiment analysis. In that case you'll use an *encoder* block with no mask, whereas we use a *decoder* block to mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c31fa-41b9-4be3-8d3c-214eff10af17",
   "metadata": {},
   "source": [
    "### Note 5: Self vs Cross-Attention\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4539s)\n",
    "\n",
    "Our architecture is called *self-attention* because the keys, queries, and values are all coming from the same source, in our case from `x`. It produces keys, queries, and values. Attention is much more general than that. You could have a situation where queries are produced from `x` but keys and values come from an external sources, for example they could come from an encoder block. Cross-attention is used when there is a separate source of nodes that we're pulling information from into our nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5b126-94ee-4da7-81a8-5c116c7dd62b",
   "metadata": {},
   "source": [
    "### Note 6: Importance of *Scaled* Self-Attention\n",
    "[Video Link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4616s)\n",
    "\n",
    "Section 3.2.1 of *[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)* addresses this and there's one other part we didn't do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa63bd-2fc5-4ac0-b6b9-42be22feeba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
